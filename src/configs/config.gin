
# LAUNCHER
run_experiment.replay_buffer_size = 10000

# SAC
init_sac_agent.learning_rate = 3e-4
init_sac_agent.target_update_tau = 0.005 # @param {type:"number"}
init_sac_agent.target_update_period = 1 # @param {type:"number"}
init_sac_agent.gamma = 0.99 # @param {type:"number"}
init_sac_agent.reward_scale_factor = 1.0 # @param {type:"number"}

init_sac_agent.actor_fc_layer_params = (256, 256)
init_sac_agent.critic_joint_fc_layer_params = (256, 256)

# SKILL DISCOVERY
train_skill_discovery.num_epochs = 50
train_skill_discovery.initial_collect_steps = 5000
train_skill_discovery.collect_steps_per_epoch = 1000  # turn into collect_episodes ?
train_skill_discovery.dynamics_train_steps_per_epoch = 32
train_skill_discovery.sac_train_steps_per_epoch = 32

# DISCRIMINATOR

# LOGGING
init_logging.log_dir = 'logs/diayn/vis'
init_logging.create_fig_interval = 1
