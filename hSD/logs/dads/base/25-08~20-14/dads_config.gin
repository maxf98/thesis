run_experiment.objective = 'sz->s_p'
run_experiment.skill_prior = 'cont_uniform'
run_experiment.skill_dim = 2

init_rollout_driver.buffer_size = 300
init_rollout_driver.skill_length = 4
init_rollout_driver.episode_length = 4

init_skill_model.hidden_dim = (64, 64)
init_skill_model.fix_variance = True  # only continuous skill models

init_policy_learner.rl_alg = 'SAC'
init_policy_learner.fc_layer_params = (64, 64)
init_policy_learner.target_entropy = 0.001
init_policy_learner.reward_scale_factor = 10.0

init_logger.create_logs = True
init_logger.log_dir = 'logs/dads/base'
init_logger.create_fig_interval = 5
init_logger.skill_length = 4
init_logger.num_samples_per_skill = 3

train_skill_discovery.num_epochs=100
train_skill_discovery.initial_collect_steps=300
train_skill_discovery.collect_steps_per_epoch=200
train_skill_discovery.train_batch_size=16
train_skill_discovery.skill_model_train_steps=32
train_skill_discovery.policy_learner_train_steps=64
