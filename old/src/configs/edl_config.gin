
# LAUNCHER
run_experiment.latent_dim = 2
run_experiment.type = "EDL"

# SAC
init_sac_agent.learning_rate = 3e-4
init_sac_agent.target_update_tau = 0.005
init_sac_agent.target_update_period = 1
init_sac_agent.gamma = 0.99
init_sac_agent.reward_scale_factor = 1.0

init_sac_agent.actor_fc_layer_params = (256, 256)
init_sac_agent.critic_joint_fc_layer_params = (256, 256)

# SKILL DISCOVERY
train_skill_discovery.num_epochs = 1
train_skill_discovery.initial_collect_steps = 30000
train_skill_discovery.collect_steps_per_epoch = 10000  # turn into collect_episodes ?
train_skill_discovery.dynamics_train_steps_per_epoch = 100
train_skill_discovery.sac_train_steps_per_epoch = 1000

# DISCRIMINATOR
init_skill_discriminator.intermediate_dim = 128

# LOGGING
init_logging.log_dir = 'logs/edl'
init_logging.create_fig_interval = 1
