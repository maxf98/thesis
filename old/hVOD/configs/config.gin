
# LAUNCHER
run_experiment.latent_dim = 2
# use_state_delta = False

# SAC
init_sac_agent.learning_rate = 3e-4
init_sac_agent.target_update_tau = 0.005 # @param {type:"number"}
init_sac_agent.target_update_period = 1 # @param {type:"number"}
init_sac_agent.gamma = 0.99 # @param {type:"number"}
init_sac_agent.reward_scale_factor = 1.0 # @param {type:"number"}

init_sac_agent.actor_fc_layer_params = (128, 128)
init_sac_agent.critic_joint_fc_layer_params = (128, 128)

# SKILL DISCOVERY
init_skill_discovery.max_skill_length = 70

train_skill_discovery.num_epochs = 500
train_skill_discovery.initial_collect_steps = 5000
train_skill_discovery.collect_steps_per_epoch = 2000  # turn into collect_episodes ?
train_skill_discovery.dynamics_train_steps_per_epoch = 32
train_skill_discovery.sac_train_steps_per_epoch = 128

# BUFFER
init_buffer.buffer_size = 5000

# DISCRIMINATOR
init_skill_discriminator.intermediate_dim = 128

# LOGGING
init_logging.log_dir = 'logs/gsd'
init_logging.create_fig_interval = 20
