
# LAUNCHER
run_experiment.latent_dim = 2
# use_state_delta = False

# SAC
init_sac_agent.learning_rate = 3e-4
init_sac_agent.target_update_tau = 0.005 # @param {type:"number"}
init_sac_agent.target_update_period = 1 # @param {type:"number"}
init_sac_agent.gamma = 0.99 # @param {type:"number"}
init_sac_agent.reward_scale_factor = 1.0 # @param {type:"number"}

init_sac_agent.actor_fc_layer_params = (128, 128)
init_sac_agent.critic_joint_fc_layer_params = (128, 128)

# SKILL DISCOVERY
train_skill_discovery.num_epochs = 100
train_skill_discovery.initial_collect_steps = 2000
train_skill_discovery.collect_steps_per_epoch = 1000  # turn into collect_episodes ?
train_skill_discovery.dynamics_train_steps_per_epoch = 64
train_skill_discovery.sac_train_steps_per_epoch = 64

# BUFFER
init_buffer.buffer_size = 4000

# DISCRIMINATOR
init_skill_discriminator.intermediate_dim = 128

# LOGGING
init_logging.log_dir = 'logs'
init_logging.create_fig_interval = 5
